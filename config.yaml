tasks:
  qa:
    available_datasets: ["squad", "natural_questions", "custom_qa_dataset"]
  summarization:
    available_datasets: ["xsum", "cnn_dailymail", "custom_summarization_dataset"]

model:
  name: "openai-community/gpt2"
  task: "qa"  # Task can be changed dynamically by the user

LoRA:
  enable: true
  rank: 8
  alpha: 16
  dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  training_args: 
        output_dir: "./outputs"
        per_device_train_batch_size: 1
        num_train_epochs: 3
        logging_steps: 10
        save_steps: 50
        eval_strategy: "epoch"
        fp16: False
        learning_rate: 0.00005
        report_to: "none"

cleaning:
  min_length: 10
  max_length: 1000
  remove_duplicates: true

preprocessing:
  lowercase: true
  remove_urls: true
  remove_extra_whitespace: true
  expand_contractions: true

splitting:
  train_ratio: 0.9
  validation_ratio: 0.1
  random_seed: 42

tokenization:
  max_length: 512
  padding: true
  truncation: true

output:
  save_cleaned: true
  save_tokenized: true
  save_stats: true
  output_dir: "output"
